{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "verified-rings",
   "metadata": {},
   "source": [
    "## Starter notebook for single-cell nuclear RNA analysis\n",
    "### from scanpy tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "touched-pepper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/software/cellgen/team274/lr26/miniforge3/envs/my-python/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/cellgen/team274/lr26/miniforge3/envs/my-python/lib/python3.10/site-packages/docrep/decorators.py:43: SyntaxWarning: 'param_categorical_covariate_keys' is not a valid key!\n",
      "  doc = func(self, args[0].__doc__, *args[1:], **kwargs)\n",
      "/software/cellgen/team274/lr26/miniforge3/envs/my-python/lib/python3.10/site-packages/docrep/decorators.py:43: SyntaxWarning: 'param_continuous_covariate_keys' is not a valid key!\n",
      "  doc = func(self, args[0].__doc__, *args[1:], **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "### load packages\n",
    "import anndata as ad # for annotating with obs and var\n",
    "import scanpy as sc # core sc package\n",
    "import pandas as pd # dataframes\n",
    "#import harmonypy as hm # for batch effect correction\n",
    "#import scanorama # for batch effect correction \n",
    "import scvi as scvi # for doublet handling\n",
    "import seaborn as sns # plots\n",
    "import matplotlib.pyplot as plt # plots\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix # matrices\n",
    "import sys # system\n",
    "import os # system\n",
    "import glob # global system\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#import torch # for training scvi\n",
    "import random\n",
    "import numpy as np # numbers\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "normal-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # Set seed for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the cellranger output - it looks for perfect transcript matches so introns not really relevant\n",
    "# Use deep learning (scVI) for doublet removal\n",
    "# Account for nuclear-specific QC metrics (lower gene counts than scRNA-seq)\n",
    "# Be cautious with mitochondrial genes (not always high in nuclei)\n",
    "# batch correction or multiple sample integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dressed-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set base directory\n",
    "base_dir = \"/lustre/scratch126/cellgen/behjati/lr26/snRNA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enhanced-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all folders ending with '_filtered_feature_bc_matrix'\n",
    "matrix_dir_list = glob.glob(os.path.join(base_dir, \"*_filtered_feature_bc_matrix\"))\n",
    "\n",
    "def get_sample_id(matrix_dir_path):\n",
    "    foldername = os.path.basename(matrix_dir_path)\n",
    "    # Assumes sample ID is first three parts separated by \"_\", e.g. CG_SB_NB13960948\n",
    "    sample_id = \"_\".join(foldername.split(\"_\")[0:3])\n",
    "    return sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elegant-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scrublet(adata, expected_rate=0.1, n_pcs=10):\n",
    "    try:\n",
    "        # Filter out cells with zero counts\n",
    "        counts_per_cell = np.array(adata.X.sum(axis=1)).flatten()\n",
    "        adata = adata[counts_per_cell > 0].copy()\n",
    "\n",
    "        sc.pp.scrublet(\n",
    "            adata,\n",
    "            expected_doublet_rate=expected_rate,\n",
    "            threshold=None,  # auto-estimate threshold\n",
    "            n_prin_comps=n_pcs,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        adata.obs[\"scrublet\"] = adata.obs[\"predicted_doublets\"].map({True: \"doublet\", False: \"singlet\"})\n",
    "        adata.obs.rename(columns={\"doublet_score\": \"scrublet_score\"}, inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Scrublet failed: {e}\")\n",
    "\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab6e35f-80e7-4751-bc32-8b5c04ef318b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CG_SB_NB13960949: (43714, 33694)\n",
      "Scrublet failed: 'predicted_doublets'\n",
      "Loaded CG_SB_NB14449540: (13184, 33694)\n",
      "Scrublet failed: 'predicted_doublets'\n",
      "Loaded CG_SB_NB13960950: (1566, 33694)\n",
      "Scrublet failed: 'predicted_doublets'\n",
      "Loaded CG_SB_NB13960948: (2084, 33694)\n",
      "Scrublet failed: 'predicted_doublets'\n",
      "Loaded CG_SB_NB14449539: (12163, 33694)\n",
      "Scrublet failed: 'predicted_doublets'\n",
      "Loaded CG_SB_NB14449541: (11743, 33694)\n",
      "Scrublet failed: 'predicted_doublets'\n",
      "Loaded CG_SB_NB13960951: (1075, 33694)\n",
      "Scrublet failed: 'predicted_doublets'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1852003/1329535910.py:28: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html\n",
      "  adata_merged = adatas[0].concatenate(*adatas[1:], index_unique=\"-\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (85529, 33694)\n"
     ]
    }
   ],
   "source": [
    "def load_and_merge_data(matrix_dir_list):\n",
    "    adatas = []\n",
    "\n",
    "    for matrix_dir in matrix_dir_list:\n",
    "        sample_id = get_sample_id(matrix_dir)\n",
    "\n",
    "        # Load the 10X MTX matrix from folder\n",
    "        adata = sc.read_10x_mtx(\n",
    "            matrix_dir,\n",
    "            var_names=\"gene_symbols\",  # or \"gene_ids\"\n",
    "            cache=True\n",
    "        )\n",
    "        adata.var_names_make_unique()\n",
    "\n",
    "        print(f\"Loaded {sample_id}: {adata.shape}\")\n",
    "\n",
    "        # Run Scrublet\n",
    "        adata = run_scrublet(adata)\n",
    "\n",
    "        # Annotate metadata\n",
    "        adata.obs[\"CellID\"] = [sample_id + \":\" + bc for bc in adata.obs_names]\n",
    "        adata.obs[\"SampleID\"] = sample_id\n",
    "        adata.obs[\"Barcode\"] = adata.obs_names\n",
    "\n",
    "        adatas.append(adata)\n",
    "\n",
    "    # Merge all samples into one AnnData object\n",
    "    adata_merged = adatas[0].concatenate(*adatas[1:], index_unique=\"-\")\n",
    "\n",
    "    print(f\"Merged data shape: {adata_merged.shape}\")\n",
    "\n",
    "    return adata_merged\n",
    "\n",
    "# Run the whole pipeline\n",
    "adata = load_and_merge_data(matrix_dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all directories ending with \"_filtered_feature_bc_matrix\"\n",
    "matrix_dirs = glob.glob(os.path.join(base_dir, \"*_filtered_feature_bc_matrix\"))\n",
    "\n",
    "# Function to extract sample ID from file name\n",
    "def get_sample_id(matrix_dir_path):\n",
    "    foldername = os.path.basename(matrix_dir_path)\n",
    "    sample_id = \"_\".join(foldername.split(\"_\")[0:3])  # CG_SB_NBxxxxxxx\n",
    "    return sample_id\n",
    "\n",
    "# Example: get sample IDs for all files\n",
    "for matrix_dir in matrix_dirs:\n",
    "    sample_id = get_sample_id(matrix_dir)\n",
    "    print(f\"{sample_id}: {matrix_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54199bd7-6264-4d16-b382-7b8fae7b6df4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 85529 Ã— 33694\n",
       "    obs: 'doublet_score', 'predicted_doublet', 'CellID', 'SampleID', 'Barcode', 'batch'\n",
       "    var: 'gene_ids', 'feature_types'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality control calculations\n",
    "# Calculate QC metricsm - do this first\n",
    "# number of genes detected per cell\n",
    "# Total UMIs per cell\n",
    "# Mitochondrial gene percentage\n",
    "# Ribosomal gene percentage\n",
    "\n",
    "adata_merged.var[\"mt_gene\"] = adata_merged.var_names.str.startswith((\"MT-\", \"mt-\"))  # mitochondrial genes, handles anything starting with mt\n",
    "adata_merged.var[\"ribo_gene\"] = adata_merged.var_names.str.contains((\"^RP[SL][0-9]\"), regex=True)  # ribosomal genes anything that contains the regex\n",
    "#calculate quality metrics\n",
    "sc.pp.calculate_qc_metrics(adata_merged, qc_vars=[\"mt_gene\", \"ribo_gene\"], percent_top=None, log1p=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of the counts with violin plot\n",
    "sc.pl.violin(adata_merged, [\"n_genes_by_counts\", \"total_counts\"], jitter=0.4)\n",
    "sc.pl.violin(adata_merged, [\"pct_counts_mt_gene\", \"pct_counts_ribo_gene\"], jitter=0.4)\n",
    "sc.pl.scatter(adata_merged, x=\"total_counts\", y=\"n_genes_by_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata_merged) # how does it look\n",
    "print(adata_merged.obs.head()) #cells\n",
    "print(adata_merged.var.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata_merged) # how does it look\n",
    "print(adata_merged.obs.head()) #cells\n",
    "print(adata_merged.var.head())\n",
    "print(adata_merged.shape)  # (n_cells, n_genes)\n",
    "type(adata_merged.X) # what type of data is it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adata_x.X[1:20,15:40].todense()\n",
    "print(adata_merged.X[1:20, 15:40].toarray())  # safer for memory to have a look at the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run PCA (if you haven't already, as UMAP relies on PCA or other dimensionality reduction)\n",
    "sc.tl.pca(adata_merged, svd_solver='arpack')\n",
    "\n",
    "# Step 2: Compute the nearest neighbors graph\n",
    "sc.pp.neighbors(adata_merged, n_neighbors=15, n_pcs=50)  # n_neighbors can be adjusted\n",
    "\n",
    "# Step 3: Calculate the UMAP embeddings\n",
    "sc.tl.umap(adata_merged)\n",
    "\n",
    "# Step 4: Now you can visualize the UMAP plot with 'scrublet' and 'scrublet_score' colorings\n",
    "sc.pl.umap(adata_merged, color=[\"scrublet\", \"scrublet_score\"])\n",
    "\n",
    "#Step 2: Visualize doublets vs singlets based on library size (total counts)\n",
    "sc.pl.scatter(adata_merged, x=\"total_counts\", y=\"n_genes_by_counts\", color=\"scrublet\", title=\"Doublets vs Singlets based on Library Size\", palette=[\"grey\", \"red\"])\n",
    "\n",
    "# You can also use a UMAP plot if you have UMAP coordinates\n",
    "sc.pl.umap(adata_merged, color=\"scrublet\", title=\"Doublets vs Singlets on UMAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(adata_merged.obs[\"scrublet_score\"], bins=50, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each category in the 'scrublet' column\n",
    "doublet_counts = adata_merged.obs[\"scrublet\"].value_counts()\n",
    "# Print the count of doublets\n",
    "print(f\"Number of doublets identified: {doublet_counts['doublet']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_previous = sc.read_h5ad(base_dir + \"adata_merged_after_scrumblet.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_merged.write(base_dir + \"adata_merged_new_env_scrublet.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new = sc.read_h5ad(base_dir + \"adata_merged_after_scrumblet_scvi.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata_scrscvi) # how does it look\n",
    "print(adata_scrscvi.obs.head()) #cells\n",
    "print(adata_scrscvi.var.head())\n",
    "print(adata_scrscvi.shape)  # (n_cells, n_genes)\n",
    "type(adata_scrscvi.X) # what type of data is it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata_previous.obs.index.duplicated().sum())  # Should be 0\n",
    "print(adata_new.obs.index.duplicated().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each category in the 'scrublet' column\n",
    "doublet_counts = adata_previous.obs[\"scrublet\"].value_counts()\n",
    "# Print the count of doublets\n",
    "print(f\"Number of doublets identified: {doublet_counts['doublet']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to a dense for checking\n",
    "X_dense = adata_merged.X.toarray() if sp.issparse(adata_merged.X) else adata_merged.X\n",
    "# NaN values - none there\n",
    "print(\"NaN values in expression matrix:\", np.isnan(X_dense).sum())\n",
    "# infinite values - none there\n",
    "print(\"Infinite values in expression matrix:\", np.isinf(X_dense).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new.layers[\"counts\"] = adata_new.X.copy()\n",
    "#adata_previous.layers[\"counts\"] = adata_previous.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all CPU cores for PyTorch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "def run_scvi_solo(adata, n_epochs=50, use_gpu=False):\n",
    "    \"\"\"Runs SCVI and SOLO on a given AnnData object.\"\"\"\n",
    "    \n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    adata_copy = adata.copy()\n",
    "    \n",
    "    # SCVI setup for AnnData\n",
    "    scvi.model.SCVI.setup_anndata(\n",
    "        adata_copy, \n",
    "        batch_key=\"SampleID\",  # Specify batch correction key\n",
    "        layer=\"counts\"  # Raw counts should be stored in the 'counts' layer\n",
    "    )\n",
    "    \n",
    "    # Train SCVI model\n",
    "    vae = scvi.model.SCVI(adata_copy)\n",
    "    vae.train(max_epochs=n_epochs)\n",
    "\n",
    "    # Run SOLO for doublet detection\n",
    "    solo = scvi.external.SOLO.from_scvi_model(vae)\n",
    "    solo.train(max_epochs=n_epochs)\n",
    "\n",
    "    # Get SOLO predictions\n",
    "    df = solo.predict()\n",
    "    df[\"prediction\"] = solo.predict(soft=False)\n",
    "    df[\"dif\"] = df.doublet - df.singlet  # Confidence score\n",
    "    df.index = df.index.map(lambda x: x[:-2])  # Adjust cell names if needed\n",
    "\n",
    "    # Store results in the AnnData object\n",
    "    adata_copy.obs[\"scVI_SOLO\"] = df[\"prediction\"]\n",
    "    adata_copy.obs[\"scVI_SOLO_score\"] = df[\"dif\"]\n",
    "    \n",
    "    return adata_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scvi_solo(adata, n_epochs=50, use_gpu=False):\n",
    "    \"\"\"Runs SCVI and SOLO on a given AnnData object.\"\"\"\n",
    "    \n",
    "    # Create a copy of the dataset\n",
    "    adata_copy = adata.copy()\n",
    "    \n",
    "    # Ensure unique cell names\n",
    "    if adata_copy.obs.index.duplicated().sum() > 0:\n",
    "        print(\"âš ï¸ Warning: Duplicate cell names detected. Fixing them...\")\n",
    "        adata_copy.obs.index = adata_copy.obs.index + \"_\" + adata_copy.obs.groupby(level=0).cumcount().astype(str)\n",
    "\n",
    "    # SCVI setup for AnnData\n",
    "    scvi.model.SCVI.setup_anndata(\n",
    "        adata_copy, \n",
    "        batch_key=\"SampleID\",\n",
    "        layer=\"counts\"\n",
    "    )\n",
    "    \n",
    "    # Train SCVI\n",
    "    vae = scvi.model.SCVI(adata_copy)\n",
    "    vae.train(max_epochs=n_epochs)\n",
    "\n",
    "    # Train SOLO\n",
    "    solo = scvi.external.SOLO.from_scvi_model(vae)\n",
    "    solo.train(max_epochs=n_epochs)\n",
    "\n",
    "    # Get predictions\n",
    "    df = solo.predict()\n",
    "    df[\"prediction\"] = solo.predict(soft=False)\n",
    "    df[\"dif\"] = df.doublet - df.singlet\n",
    "\n",
    "    # Ensure SOLO's cell names match AnnData\n",
    "    df.index = df.index.str.replace(\"_1$\", \"\", regex=True)  # Adjust if needed\n",
    "\n",
    "    # Match indices before assignment\n",
    "    df = df.loc[df.index.intersection(adata_copy.obs.index)]  # Keep only matching cells\n",
    "\n",
    "    # Store results\n",
    "    adata_copy.obs.loc[df.index, \"scVI_SOLO\"] = df[\"prediction\"]\n",
    "    adata_copy.obs.loc[df.index, \"scVI_SOLO_score\"] = df[\"dif\"]\n",
    "    \n",
    "    return adata_copy\n",
    "\n",
    "### Run scVI-SOLO on both datasets\n",
    "solo_results_new = run_scvi_solo(adata_new, n_epochs=50)\n",
    "\n",
    "# Append results to the original datasets\n",
    "adata_new.obs[\"scVI_SOLO\"] = solo_results_new.obs[\"scVI_SOLO\"]\n",
    "adata_new.obs[\"scVI_SOLO_score\"] = solo_results_new.obs[\"scVI_SOLO_score\"]\n",
    "\n",
    "adata_new.write(base_dir + \"adata_merged_new_after_scrumblet_scvi.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run scVI-SOLO on both datasets\n",
    "solo_results_previous = run_scvi_solo(adata_previous, n_epochs=50)\n",
    "\n",
    "# Append results to the original datasets\n",
    "adata_previous.obs[\"scVI_SOLO\"] = solo_results_previous.obs[\"scVI_SOLO\"]\n",
    "adata_previous.obs[\"scVI_SOLO_score\"] = solo_results_previous.obs[\"scVI_SOLO_score\"]\n",
    "\n",
    "adata_previous.write(base_dir + \"adata_merged_after_scrumblet_scvi.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run scVI-SOLO on both datasets\n",
    "solo_results_new = run_scvi_solo(adata_new, n_epochs=50)\n",
    "\n",
    "# Append results to the original datasets\n",
    "adata_new.obs[\"scVI_SOLO\"] = solo_results_new.obs[\"scVI_SOLO\"]\n",
    "adata_new.obs[\"scVI_SOLO_score\"] = solo_results_new.obs[\"scVI_SOLO_score\"]\n",
    "\n",
    "adata_new.write(base_dir + \"adata_merged_new_after_scrumblet_scvi.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi = sc.read_h5ad(base_dir + \"adata_merged_after_scrumblet_scvi.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata_scvi) # how does it look\n",
    "print(adata_scvi.obs.head()) #cells\n",
    "print(adata_scvi.var.head())\n",
    "print(adata_scvi.shape)  # (n_cells, n_genes)\n",
    "type(adata_scvi.X) # what type of data is it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "solo_results_merged = run_scvi_solo(adata_merged, n_epochs=50)\n",
    "adata_merged.obs[\"scVI_SOLO\"] = solo_results_merged.obs[\"scVI_SOLO\"]\n",
    "adata_merged.obs[\"scVI_SOLO_score\"] = solo_results_merged.obs[\"scVI_SOLO_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append results to the original datasets\n",
    "adata_previous.obs[\"scVI_SOLO\"] = solo_results_previous.obs[\"scVI_SOLO\"]\n",
    "adata_previous.obs[\"scVI_SOLO_score\"] = solo_results_previous.obs[\"scVI_SOLO_score\"]\n",
    "\n",
    "adata_merged.obs[\"scVI_SOLO\"] = solo_results_merged.obs[\"scVI_SOLO\"]\n",
    "adata_merged.obs[\"scVI_SOLO_score\"] = solo_results_merged.obs[\"scVI_SOLO_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata_merged.X.shape)  # Check the shape of the main data matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata_merged, target_sum=1e4)  # normalize per cell\n",
    "sc.pp.log1p(adata_merged)  # Log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata_x.obs.groupby(\"scVI_SOLO\").size())\n",
    "print(adata_x.obs.groupby(\"scrublet\").size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”¹ Filter out doublets from `adata_merged`\n",
    "adata_filtered = adata_merged[adata_merged.obs[\"scVI_SOLO\"] == \"singlet\", :]\n",
    "\n",
    "# âœ… Save the filtered dataset\n",
    "adata_filtered.write(base_dir + \"adata_filtered.h5ad\")\n",
    "\n",
    "# ðŸ”¹ Check how many cells were removed\n",
    "print(f\"Original: {adata_merged.shape}, Filtered: {adata_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) Extract latent embeddings\n",
    "adata_filtered.obsm[\"X_scVI\"] = model.get_latent_representation()\n",
    "\n",
    "# (8) Run UMAP/clustering\n",
    "sc.pp.neighbors(adata_filtered, use_rep=\"X_scVI\")\n",
    "sc.tl.umap(adata_filtered)\n",
    "sc.tl.leiden(adata_filtered, resolution=0.5)\n",
    "\n",
    "# (9) Visualize\n",
    "sc.pl.umap(adata_filtered, color=[\"SampleID\", \"leiden\", \"scrublet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets\n",
    "adata_merged = adatas[0].concatenate(*adatas[1:], batch_key=\"SampleID\", index_unique=\"-\")\n",
    "\n",
    "# Check for duplicates based on barcodes\n",
    "# Assuming barcodes are stored in the obs_names column or an explicit 'Barcode' column\n",
    "# Remove duplicates based on barcode (if necessary)\n",
    "adata_merged = adata_merged[~adata_merged.obs_names.duplicated(), :]\n",
    "\n",
    "# Alternatively, you can remove duplicates based on CellID if that's a more appropriate column\n",
    "adata_merged = adata_merged[~adata_merged.obs[\"CellID\"].duplicated(), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter dying cells\n",
    "adata = adata[adata.obs.n_genes_by_counts > 200, :]  # Remove nuclei with very few genes\n",
    "adata = adata[adata.obs.total_counts < 100000, :]  # Remove outliers\n",
    "adata = adata[adata.obs.pct_counts_mt < 5, :]  # Remove cells with >5% mito reads\n",
    "adata = adata[adata.obs.pct_counts_ribo < 10, :]  # Remove high ribosomal content\n",
    "\n",
    "# Set thresholds (adjust based on dataset)\n",
    "min_genes = 200  # Keep cells with at least 200 genes\n",
    "max_mito = 5     # Remove cells with >5% mitochondrial content\n",
    "\n",
    "# Filter low-quality cells\n",
    "adata = adata[(adata.obs.n_genes_by_counts > min_genes) & \n",
    "              (adata.obs.pct_counts_mt < max_mito)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation of the data\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=3000)\n",
    "adata = adata[:, adata.var.highly_variable]  # Keep only highly variable genes\n",
    "sc.pp.scale(adata, max_value=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction for batch effects\n",
    "# Compute PCA\n",
    "sc.tl.pca(adata, svd_solver=\"arpack\")\n",
    "# Run Harmony batch correction\n",
    "harmony = hm.run_harmony(adata.obsm[\"X_pca\"], adata.obs, \"batch\")\n",
    "adata.obsm[\"X_pca_harmony\"] = harmony.Z_corr.T  # Use corrected PCA embeddings\n",
    "# Recompute neighbors and UMAP on corrected embeddings\n",
    "sc.pp.neighbors(adata, use_rep=\"X_pca_harmony\")\n",
    "sc.tl.umap(adata)\n",
    "# Visualize batch correction effect\n",
    "sc.pl.umap(adata, color=[\"batch\", \"cell_type\"])\n",
    "\n",
    "import scanorama\n",
    "# Split dataset by batch\n",
    "batches = [adata[adata.obs.batch == b] for b in adata.obs.batch.unique()]\n",
    "# Apply Scanorama integration\n",
    "integrated_batches, corrected_matrices = scanorama.correct_scanpy(batches)\n",
    "# Merge back into one AnnData object\n",
    "adata_corrected = sc.AnnData.concatenate(*integrated_batches, batch_key=\"batch\")\n",
    "# Run PCA & clustering\n",
    "sc.tl.pca(adata_corrected)\n",
    "sc.pp.neighbors(adata_corrected)\n",
    "sc.tl.umap(adata_corrected)\n",
    "# Visualize\n",
    "sc.pl.umap(adata_corrected, color=[\"batch\", \"cell_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(adata, groupby=\"cell_type\", method=\"wilcoxon\")\n",
    "sc.pl.rank_genes_groups(adata, n_genes=10, sharey=False)\n",
    "\n",
    "df = sc.get.rank_genes_groups_df(adata, group=\"Neurons\")\n",
    "df.to_csv(\"DEGs_Neurons.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write(\"snRNA_final.h5ad\")\n",
    "sc.pl.umap(adata, color=[\"cell_type\", \"n_genes_by_counts\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Holly code\n",
    "def run_scrublet(adata_x):\n",
    "    \n",
    "    scrub = scr.Scrublet(adata_x.X.todense())\n",
    "    \n",
    "    try:\n",
    "        doublet_scores, predicted_doublets = scrub.scrub_doublets()\n",
    "        adata_x.obs[\"scrublet\"] = [\"doublet\" if i == True else \"singlet\" for i in predicted_doublets]\n",
    "        adata_x.obs[\"scrublet_score\"] = doublet_scores\n",
    "        return adata_x\n",
    "    except:\n",
    "        print(\"Scrublet failed\")\n",
    "        return adata_x\n",
    "\n",
    "    ##### ----- Add scrublet per patient\n",
    "    adata_x = run_scrublet(adata_x)\n",
    "    qc_data_x  = adata_x.obs\n",
    "\n",
    "    ## -- Filter cells\n",
    "    print(sample_x)\n",
    "    adata_x.var[\"mito\"] = adata_x.var_names.str.startswith(\"MT-\")\n",
    "    sc.pp.calculate_qc_metrics(adata_x, qc_vars=[\"mito\"], inplace=True)\n",
    "    gene_thresh = 500\n",
    "    mito_thresh = 15\n",
    "    to_discard = np.array(qc_data_x[\"pct_counts_mito\"]>mito_thresh) | np.array(qc_data_x[\"n_genes_by_counts\"]<float(gene_thresh))\n",
    "    cells_to_keep = list(qc_data_x.loc[~to_discard][\"CellID\"])\n",
    "    \n",
    "    print(\"Before: \"+str(adata_x.shape[0])+\" cells\")\n",
    "    adata_x = adata_x[adata_x.obs_names.isin(cells_to_keep)]\n",
    "    print(\"After: \"+str(adata_x.shape[0])+\" cells\")\n",
    "    \n",
    "    return adata_x\n",
    "\n",
    "adat_lst = [loadAnnData(i) for i in folders_to_load]\n",
    "adata = ad.concat(adat_lst,join=\"outer\", label=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a926177-c89c-4428-81d7-61db97b19721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d18ff-227e-4fe9-99d3-197edecae5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc885c-adac-42fc-82e8-ee9d6d782b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542297a-c364-43f5-8e25-63dcf53421e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473715e-8340-44c2-9fce-3a9b72eca4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406dd5c8-e2d1-439f-a481-01dafa20721a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(base_dir + \"adata_merged_scrublet_scvi_umi_cons_doub.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279b4cd-9d82-4418-b6af-2d4d74d653eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db11f7f-8d3d-4891-9824-0fce7b7ba9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAD outlier detection\n",
    "def mad_outlier(adata, metric, nmads):\n",
    "    M = adata.obs[metric]\n",
    "    return (M < np.median(M) - nmads * mad(M)) | (M > np.median(M) + nmads * mad(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95936f-dcf7-4e51-a9df-6a551a7cf56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Soupx groups\n",
    "def get_soupx_group(adata):\n",
    "    adata_pp = adata.copy()\n",
    "    sc.pp.normalize_per_cell(adata_pp)\n",
    "    sc.pp.log1p(adata_pp)\n",
    "    sc.pp.pca(adata_pp)\n",
    "    sc.pp.neighbors(adata_pp)\n",
    "    sc.tl.leiden(adata_pp, key_added=\"soupx_groups\")\n",
    "    return adata_pp.obs['soupx_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b8088-7b96-4faa-9dcd-046c27dcd2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare broth data for SoupX\n",
    "def prepare_broth(adata):\n",
    "    cells = adata.obs_names\n",
    "    genes = adata.var_names\n",
    "    data = adata.X.T\n",
    "    \n",
    "    # Get raw data (assuming it's stored separately in your case)\n",
    "    sample_id = adata.obs.iloc[0]['SampleID']\n",
    "    raw = sc.read_10x_mtx(\"/lustre/scratch126/casm/team274sb/lr26/scRNA/\" + sample_id + \"/raw_feature_bc_matrix/\").X.T\n",
    "    \n",
    "    # Get leiden clusters\n",
    "    soupx_groups = get_soupx_group(adata)\n",
    "\n",
    "    return data, raw, genes, cells, soupx_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60176e57-8e51-4271-94ec-d0e8482d6f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to run SoupX analysis in R\n",
    "import rpy2.robjects as r\n",
    "\n",
    "# Define the R function (inside Python)\n",
    "r.r(\"\"\"\n",
    "    library(SoupX)\n",
    "\n",
    "    make_soup <- function(data, raw, genes, cells, soupx_groups){\n",
    "        rownames(data) <- genes\n",
    "        colnames(data) <- cells\n",
    "        data <- as(data, \"sparseMatrix\")\n",
    "        raw <- as(raw, \"sparseMatrix\")\n",
    "        \n",
    "        sc <- SoupChannel(raw, data, calcSoupProfile = FALSE)\n",
    "        \n",
    "        soupProf <- data.frame(row.names = rownames(data), est = rowSums(data)/sum(data), counts = rowSums(data))\n",
    "        sc <- setSoupProfile(sc, soupProf)\n",
    "        \n",
    "        sc <- setClusters(sc, soupx_groups)\n",
    "        \n",
    "        sc <- autoEstCont(sc, tfidfMin = 0.1, soupQuantile = 0.1, doPlot=FALSE, forceAccept=TRUE)\n",
    "        out <- adjustCounts(sc, roundToInt = TRUE)\n",
    "\n",
    "        return(out)\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9646874-af86-44f8-904c-4adad2f38c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to cook soup and add layers\n",
    "def cook_soup(adata):\n",
    "    data, raw, genes, cells, soupx_groups = prepare_broth(adata)\n",
    "\n",
    "    # Execute the R code and get the corrected counts\n",
    "    %R -i data -i raw -i genes -i cells -i soupx_groups -o out out = make_soup(data, raw, genes, cells, soupx_groups)\n",
    "\n",
    "    # Store raw counts and SoupX counts\n",
    "    adata.layers[\"raw_counts\"] = adata.X\n",
    "    adata.layers[\"soupX_counts\"] = out.T\n",
    "    adata.X = adata.layers[\"soupX_counts\"]\n",
    "    \n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00644644-13f1-46a8-86fd-62e6ac1311a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: Process your large AnnData object\n",
    "def process_samples(adata):\n",
    "    obsm_global = adata.obsm.copy()  # Keep original global 'obsm' (embeddings like PCA, UMAP)\n",
    "    varm_global = adata.varm.copy()  # Keep original global 'varm' (e.g., PCA loadings)\n",
    "    uns_global = adata.uns.copy()    # Keep original global 'uns' (metadata, neighbors, etc.)\n",
    "    var_global = adata.var.copy() \n",
    "    obsp_global = adata.obsp.copy() # Keep the 'var' data for genes\n",
    "    combined_adata = []\n",
    "    sample_ids = adata.obs['SampleID'].unique()\n",
    "\n",
    "    for sample_id in sample_ids:\n",
    "        # Extract individual sample's AnnData object\n",
    "        adata_sample = adata[adata.obs['SampleID'] == sample_id].copy()\n",
    "        adata_sample.var = adata.var.copy()  # Ensure 'var' is kept as-is\n",
    "        adata_sample.uns = adata.uns.copy()  # Copy the 'uns' metadata\n",
    "        \n",
    "        # Run your analysis (e.g., SoupX or other preprocessing here)\n",
    "        adata_sample = cook_soup(adata_sample)  # Assuming cook_soup is defined elsewhere\n",
    "    \n",
    "        # Append processed data to the combined list\n",
    "        combined_adata.append(adata_sample)\n",
    "\n",
    "    # Combine all individual AnnData objects back into one\n",
    "    adata_combined = ad.concat(combined_adata, join='outer', label='SampleID')\n",
    "    adata_combined.obsm = obsm_global\n",
    "    adata_combined.varm = varm_global\n",
    "    adata_combined.uns = uns_global\n",
    "    adata_combined.var = var_global\n",
    "    adata_combined.obsp = obsp_global\n",
    "    # Optionally, if you need to add or modify attributes after concatenation, you can do so here\n",
    "    # For example:\n",
    "    # adata_combined.uns['some_key'] = some_value\n",
    "\n",
    "    return adata_combined\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1098bf1-5a73-4370-a272-4a998b48acca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: Process your large AnnData object\n",
    "adata_combined = process_samples(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4e020-6b8e-4a7b-8099-3039a03f3ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata_combined.write(base_dir + \"adata_merged_scrublet_scvi_umi_cons_doub_soupx.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5feb4e-79b4-4cb4-bbb2-493b1f727556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata_combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Py",
   "language": "python",
   "name": "my-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
